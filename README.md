# Multi-Source Local Search MCP Server

A standalone, offline search server implementing the Model Context Protocol (MCP). This server enables AI assistants to search through **Wikipedia (static, large-scale knowledge)** and **your local files (dynamic, personal knowledge)** without requiring external API calls or internet connectivity.

[æ—¥æœ¬èªç‰ˆ README ã¯ã“ã¡ã‚‰](#æ—¥æœ¬èªç‰ˆ)

## Features

- **Multi-Source Search**: Search across Wikipedia AND your local files (Markdown, text) simultaneously
- **Hybrid Search**: Combines BM25 (keyword matching) + Vector embeddings (semantic similarity) for best results
- **Smart Indexing**: Wikipedia index cached permanently, local files scanned on startup for latest changes
- **Completely Offline**: No external API dependencies (Google Search, etc.)
- **Free & Fast**: Uses efficient algorithms for both keyword and semantic search
- **MCP Compatible**: Works with any MCP-compatible client (Claude Desktop, etc.)
- **Ollama Integration**: Includes test client for Ollama-based agents
- **Easy Setup**: Simple installation with `uv` package manager

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama    â”‚ â—„â”€â”€â”€â”€â”€â”€ â”‚  MCP Client      â”‚ â—„â”€â”€â”€â”€â”€â”€ â”‚   Human     â”‚
â”‚   (LLM)     â”‚         â”‚  (test script)   â”‚         â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â”‚ MCP Protocol
                                â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  MCP Server      â”‚
                        â”‚  (src/server.py) â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â–¼                         â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Wikipedia Indexerâ”‚      â”‚ Local File       â”‚
         â”‚ (Static/Cached)  â”‚      â”‚ Indexer (Dynamic)â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚                         â”‚
                   â–¼                         â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ BM25 + Vector DB â”‚      â”‚ BM25 + Vector DB â”‚
         â”‚ (1M+ articles)   â”‚      â”‚ (Your files)     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Composite Pattern**: Results from both sources are merged using Reciprocal Rank Fusion (RRF) for optimal ranking.

## Installation

### Prerequisites

- Python 3.10 or higher
- [uv](https://github.com/astral-sh/uv) package manager
- (Optional) Ollama with a tool-compatible model (e.g., command-r) for testing

### Setup

1. Clone the repository:
```bash
git clone https://github.com/yourusername/localsearch-mcp.git
cd localsearch-mcp
```

2. Install dependencies:
```bash
uv sync
```

3. Build the Wikipedia index (first run only):
```bash
# Set smaller subset for testing (optional)
export WIKI_SUBSET_SIZE=10000  # Default: 1,000,000

uv run python -m src
# Press Ctrl+C after index is built
```

This will download English Wikipedia and create:
- **BM25 index** (keyword search) in `data/wiki_index.pkl`
- **Vector index** (semantic search) in `data/chroma_db/`

The initial build downloads documents and generates embeddings, which takes time. Default: 1M articles (~5GB). Full dataset: 6.8M articles (~20GB).

4. (Optional) Enable local file search:
```bash
# Set the path to your local documents
export LOCAL_DOCS_PATH="/path/to/your/notes"  # e.g., ~/ObsidianVault/Research
```

This enables searching through your:
- Markdown files (`.md`)
- Text files (`.txt`)
- Any personal notes or documentation

The server will scan this directory on each startup to index the latest content.

## Usage

### Running the MCP Server

```bash
# Without local files
uv run python -m src

# With local files
LOCAL_DOCS_PATH="/path/to/your/notes" uv run python -m src
```

The server will:
1. Load the pre-built Wikipedia index (cached, fast)
2. Scan and index local files if `LOCAL_DOCS_PATH` is set (quick for typical document collections)
3. Start listening for MCP requests on stdio
4. Provide search tools: `search`, `search_wikipedia`, and `search_local`

### Testing with Ollama

#### Simple Test (No LLM)
```bash
uv run tests/verify_with_ollama.py --simple
```

This tests the MCP connection and performs a direct search.

#### Full Agent Test (Requires Ollama)
```bash
# Make sure Ollama is running with a tool-compatible model
ollama pull command-r
ollama serve

# In another terminal:
uv run tests/verify_with_ollama.py
```

Expected output:
```
ğŸ¤– Starting MCP Client and connecting to Local Search Server...
âœ… Connected. Available tools: ['search_wikipedia']

ğŸ‘¤ User Query: Pythonã¨ã„ã†ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã®æ­´å²ã«ã¤ã„ã¦ã€ç°¡æ½”ã«æ•™ãˆã¦
ğŸ› ï¸  Agent requested 1 tool call(s)
   â†’ Tool: search_wikipedia
   â†’ Args: {'query': 'history of python programming language'}
   â†’ Output length: 1523 chars

ğŸ¤– Agent Answer:
Python was created by Guido van Rossum in the late 1980s...
```

### Integration with Claude Desktop

Add this to your Claude Desktop MCP configuration:

**Wikipedia only:**
```json
{
  "mcpServers": {
    "local-search": {
      "command": "uv",
      "args": ["run", "python", "-m", "src"],
      "cwd": "/path/to/localsearch-mcp"
    }
  }
}
```

**Wikipedia + Local Files:**
```json
{
  "mcpServers": {
    "local-search": {
      "command": "uv",
      "args": ["run", "python", "-m", "src"],
      "cwd": "/path/to/localsearch-mcp",
      "env": {
        "LOCAL_DOCS_PATH": "/Users/yourname/Documents/Notes"
      }
    }
  }
}
```

Then restart Claude Desktop and you can search both Wikipedia and your personal files in conversations!

## Project Structure

```
localsearch-mcp/
â”œâ”€â”€ pyproject.toml          # Dependencies and project metadata
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ data/                   # Index storage (created on first run)
â”‚   â”œâ”€â”€ .gitkeep
â”‚   â””â”€â”€ wiki_index.pkl      # BM25 index (not in git)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __main__.py         # Entry point for `python -m src`
â”‚   â”œâ”€â”€ server.py           # MCP server implementation
â”‚   â””â”€â”€ indexer.py          # BM25 indexing logic
â””â”€â”€ tests/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ verify_with_ollama.py  # Ollama integration test client
```

## Available Tools

### `search` (Multi-Source)

Search across Wikipedia AND your local files simultaneously using hybrid search.

**Parameters:**
- `query` (string, required): Search keywords or question
- `top_k` (integer, optional): Number of results to return per source (default: 5, max: 20)
- `strategy` (string, optional): Search strategy - `"hybrid"` (default), `"keyword"`, or `"semantic"`
- `source` (string, optional): Data source - `"all"` (default), `"wikipedia"`, or `"local"`

**Source Options:**
- **`"all"`** (default): Search both Wikipedia and local files for comprehensive results
- **`"wikipedia"`**: Search only Wikipedia (general knowledge)
- **`"local"`**: Search only your local files (personal knowledge)

**Search Strategies:**
- **`"hybrid"`** (recommended): Combines keyword matching and semantic similarity for best results
- **`"keyword"`**: Traditional BM25 keyword search (exact word matching, fast)
- **`"semantic"`**: Vector similarity search (finds conceptually similar content, even without exact words)

**Returns:**
Formatted search results with titles, URLs/paths, and content snippets. Results from both sources are merged intelligently using Reciprocal Rank Fusion (RRF).

### `search_wikipedia`

Search English Wikipedia only using hybrid search (BM25 + Vector embeddings). Convenience wrapper for `search` with `source="wikipedia"`.

**Parameters:**
- `query` (string, required): Search keywords or question
- `top_k` (integer, optional): Number of results to return (default: 3, max: 10)
- `strategy` (string, optional): Search strategy - `"hybrid"` (default), `"keyword"`, or `"semantic"`

### `search_local`

Search your local files only using hybrid search. Convenience wrapper for `search` with `source="local"`.

**Parameters:**
- `query` (string, required): Search keywords or question
- `top_k` (integer, optional): Number of results to return (default: 5, max: 20)
- `strategy` (string, optional): Search strategy - `"hybrid"` (default), `"keyword"`, or `"semantic"`

**Examples:**
```python
# Hybrid search (best results, default)
result = await session.call_tool(
    "search_wikipedia",
    arguments={"query": "python programming language", "top_k": 3}
)

# Keyword-only search (fast, exact matches)
result = await session.call_tool(
    "search_wikipedia",
    arguments={"query": "python programming language", "strategy": "keyword"}
)

# Semantic search (finds similar concepts)
result = await session.call_tool(
    "search_wikipedia",
    arguments={"query": "snake that inspired a programming language", "strategy": "semantic"}
)
```

## Customization

### Using Simple English Wikipedia (for development)

For faster development/testing, use the lightweight Simple English Wikipedia:

Edit `src/indexer.py`:
```python
# Change this line:
ds = load_dataset("wikimedia/wikipedia", "20231101.en", split="train")

# To (Simple English, limited to 10k articles):
ds = load_dataset("wikimedia/wikipedia", "20231101.simple", split="train[:10000]")
```

This reduces disk space to ~500MB and builds in a few minutes.

### Adjusting Index Size

You can limit the number of articles for testing:

```python
# Limit to 1000 articles
ds = load_dataset("wikimedia/wikipedia", "20231101.en", split="train[:1000]")
```

## Development

### Running Tests
```bash
# Simple MCP connection test
uv run tests/verify_with_ollama.py --simple

# Full Ollama agent test
uv run tests/verify_with_ollama.py
```

### Rebuilding Index
Delete `data/wiki_index.pkl` and restart the server.

## Troubleshooting

### Index Not Building
- Check disk space (needs ~500MB for Simple Wikipedia, ~20GB for full)
- Ensure stable internet connection for initial download
- Check Python version (3.10+ required)

### Ollama Connection Fails
- Verify Ollama is running: `ollama list`
- Ensure a tool-compatible model is installed: `ollama pull command-r`
- Check Ollama API is accessible: `curl http://localhost:11434`

### MCP Server Not Starting
- Check dependencies: `uv sync`
- Verify Python path in MCP config
- Check for port conflicts

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

MIT License - see LICENSE file for details

---

# æ—¥æœ¬èªç‰ˆ

## æ¦‚è¦

ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å‹•ä½œã™ã‚‹ Wikipedia æ¤œç´¢ MCP ã‚µãƒ¼ãƒãƒ¼ã§ã™ã€‚å¤–éƒ¨ API ã«ä¾å­˜ã›ãšã€å®Œå…¨ã«ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§å‹•ä½œã—ã¾ã™ã€‚

## ç‰¹å¾´

- **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢**: BM25ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼‰+ ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ï¼ˆæ„å‘³æ¤œç´¢ï¼‰ã®çµ„ã¿åˆã‚ã›ã§æœ€é«˜ã®çµæœã‚’æä¾›
- **å®Œå…¨ã‚ªãƒ•ãƒ©ã‚¤ãƒ³**: ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šä¸è¦
- **ç„¡æ–™ãƒ»é«˜é€Ÿ**: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨æ„å‘³ã®ä¸¡æ–¹ã«å¯¾å¿œã—ãŸåŠ¹ç‡çš„ãªæ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- **MCP äº’æ›**: Claude Desktop ãªã©ã® MCP å¯¾å¿œã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ä½¿ç”¨å¯èƒ½
- **Ollama çµ±åˆ**: Ollama ã‚’ä½¿ã£ãŸãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä»˜å±
- **ç°¡å˜ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**: `uv` ã«ã‚ˆã‚‹ç°¡å˜ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### å¿…è¦è¦ä»¶

- Python 3.10 ä»¥ä¸Š
- [uv](https://github.com/astral-sh/uv) ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
- (ã‚ªãƒ—ã‚·ãƒ§ãƒ³) ãƒ†ã‚¹ãƒˆç”¨ã® Ollama ã¨ãƒ„ãƒ¼ãƒ«å¯¾å¿œãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹: command-rï¼‰

### ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †

1. ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³:
```bash
git clone https://github.com/yourusername/localsearch-mcp.git
cd localsearch-mcp
```

2. ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:
```bash
uv sync
```

3. Wikipedia ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ï¼ˆåˆå›ã®ã¿ï¼‰:
```bash
# ãƒ†ã‚¹ãƒˆç”¨ã«å°ã•ã„ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
export WIKI_SUBSET_SIZE=10000  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1,000,000

uv run python -m src
# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å¾Œ Ctrl+C ã§çµ‚äº†
```

ã“ã‚Œã«ã‚ˆã‚Šä»¥ä¸‹ãŒä½œæˆã•ã‚Œã¾ã™ï¼š
- **BM25 ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹**ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼‰: `data/wiki_index.pkl`
- **ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹**ï¼ˆæ„å‘³æ¤œç´¢ï¼‰: `data/chroma_db/`

åˆå›æ§‹ç¯‰ã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚’è¡Œã†ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ä¸‡è¨˜äº‹ï¼ˆç´„5GBï¼‰ã€‚å®Œå…¨ç‰ˆ: 680ä¸‡è¨˜äº‹ï¼ˆç´„20GBï¼‰ã€‚

## ä½¿ã„æ–¹

### MCP ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•

```bash
uv run src/server.py
```

ã‚µãƒ¼ãƒãƒ¼ã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
1. æ§‹ç¯‰æ¸ˆã¿ Wikipedia ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿
2. æ¨™æº–å…¥å‡ºåŠ›ã§ MCP ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å¾…æ©Ÿ
3. `search_wikipedia` ãƒ„ãƒ¼ãƒ«ã‚’æä¾›

### Ollama ã‚’ä½¿ã£ãŸãƒ†ã‚¹ãƒˆ

#### ã‚·ãƒ³ãƒ—ãƒ«ãƒ†ã‚¹ãƒˆï¼ˆLLM ãªã—ï¼‰
```bash
uv run tests/verify_with_ollama.py --simple
```

MCP æ¥ç¶šã¨æ¤œç´¢æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚

#### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ†ã‚¹ãƒˆï¼ˆOllama å¿…è¦ï¼‰
```bash
# Ollama ã¨ llama3.2 ãƒ¢ãƒ‡ãƒ«ã‚’èµ·å‹•
ollama pull llama3.2
ollama serve

# åˆ¥ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§å®Ÿè¡Œ:
uv run tests/verify_with_ollama.py
```

### Claude Desktop ã¨ã®çµ±åˆ

Claude Desktop ã® MCP è¨­å®šã«ä»¥ä¸‹ã‚’è¿½åŠ :

```json
{
  "mcpServers": {
    "local-wiki-search": {
      "command": "uv",
      "args": ["run", "/çµ¶å¯¾ãƒ‘ã‚¹/localsearch-mcp/src/server.py"]
    }
  }
}
```

Claude Desktop ã‚’å†èµ·å‹•ã™ã‚‹ã¨ã€ä¼šè©±å†…ã§ Wikipedia æ¤œç´¢ãŒä½¿ãˆã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

## åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«

### `search_wikipedia`

ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆBM25 + ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ï¼‰ã‚’ä½¿ã£ã¦ Wikipedia ã‚’æ¤œç´¢ã—ã¾ã™ã€‚

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**
- `query` (æ–‡å­—åˆ—, å¿…é ˆ): æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¾ãŸã¯è³ªå•
- `top_k` (æ•´æ•°, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): è¿”ã™çµæœã®æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ã€æœ€å¤§: 10ï¼‰
- `strategy` (æ–‡å­—åˆ—, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): æ¤œç´¢æˆ¦ç•¥ - `"hybrid"` (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)ã€`"keyword"`ã€ã¾ãŸã¯ `"semantic"`

**æ¤œç´¢æˆ¦ç•¥:**
- **`"hybrid"`** (æ¨å¥¨): ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã¨æ„å‘³æ¤œç´¢ã‚’çµ„ã¿åˆã‚ã›ã¦æœ€è‰¯ã®çµæœã‚’æä¾›
- **`"keyword"`**: å¾“æ¥ã® BM25 ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆå®Œå…¨ä¸€è‡´ã€é«˜é€Ÿï¼‰
- **`"semantic"`**: ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦æ¤œç´¢ï¼ˆå˜èªãŒä¸€è‡´ã—ãªãã¦ã‚‚æ¦‚å¿µçš„ã«é¡ä¼¼ã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¤œç´¢ï¼‰

**æˆ»ã‚Šå€¤:**
ã‚¿ã‚¤ãƒˆãƒ«ã€URLã€æœ¬æ–‡ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’å«ã‚€æ¤œç´¢çµæœ

## ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### å®Œå…¨ç‰ˆ Wikipedia ã‚’ä½¿ç”¨

`src/indexer.py` ã‚’ç·¨é›†:
```python
# ã“ã®è¡Œã‚’å¤‰æ›´:
ds = load_dataset("wikipedia", "20220301.simple", split="train[:10000]")

# ä»¥ä¸‹ã«å¤‰æ›´:
ds = load_dataset("wikipedia", "20231101.en", split="train")
```

æ³¨: ç´„20GB ã®ãƒ‡ã‚£ã‚¹ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã¨é•·ã„æ§‹ç¯‰æ™‚é–“ãŒå¿…è¦ã§ã™ã€‚

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ§‹ç¯‰ã•ã‚Œãªã„
- ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã‚’ç¢ºèªï¼ˆSimple ç‰ˆã§ç´„500MBã€å®Œå…¨ç‰ˆã§ç´„20GBå¿…è¦ï¼‰
- åˆå›ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèª
- Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªï¼ˆ3.10ä»¥ä¸Šå¿…è¦ï¼‰

### Ollama æ¥ç¶šã‚¨ãƒ©ãƒ¼
- Ollama ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèª: `ollama list`
- llama3.2 ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª: `ollama pull llama3.2`
- Ollama API ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹ç¢ºèª: `curl http://localhost:11434`

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT License
