# Multi-Source Local Search MCP Server

A standalone, offline search server implementing the Model Context Protocol (MCP). This server enables AI assistants to search through **Wikipedia (static, large-scale knowledge)** and **your local files (dynamic, personal knowledge)** without requiring external API calls or internet connectivity.

[æ—¥æœ¬èªç‰ˆ README ã¯ã“ã¡ã‚‰](#æ—¥æœ¬èªç‰ˆ)

## Features

- **Multi-Source Search**: Search across Wikipedia AND your local files (Markdown, text) simultaneously
- **Hybrid Search**: Combines BM25 (keyword matching) + Vector embeddings (semantic similarity) for best results
- **Smart Indexing**: Wikipedia index cached permanently, local files scanned on startup for latest changes
- **Completely Offline**: No external API dependencies (Google Search, etc.)
- **Free & Fast**: Uses efficient algorithms for both keyword and semantic search
- **MCP Compatible**: Works with any MCP-compatible client (Claude Desktop, etc.)
- **Ollama Integration**: Includes test client for Ollama-based agents
- **Easy Setup**: Simple installation with `uv` package manager

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama    â”‚ â—„â”€â”€â”€â”€â”€â”€ â”‚  MCP Client      â”‚ â—„â”€â”€â”€â”€â”€â”€ â”‚   Human     â”‚
â”‚   (LLM)     â”‚         â”‚  (test script)   â”‚         â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â”‚ MCP Protocol
                                â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  MCP Server      â”‚
                        â”‚  (src/server.py) â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â–¼                         â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Wikipedia Indexerâ”‚      â”‚ Local File       â”‚
         â”‚ (Static/Cached)  â”‚      â”‚ Indexer (Dynamic)â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚                         â”‚
                   â–¼                         â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ BM25 + Vector DB â”‚      â”‚ BM25 + Vector DB â”‚
         â”‚ (1M+ articles)   â”‚      â”‚ (Your files)     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Composite Pattern**: Results from both sources are merged using Reciprocal Rank Fusion (RRF) for optimal ranking.

## Installation

### Prerequisites

- Python 3.10 or higher
- [uv](https://github.com/astral-sh/uv) package manager
- (Optional) Ollama with a tool-compatible model (e.g., command-r) for testing

### Setup

1. Clone the repository:
```bash
git clone https://github.com/yourusername/localsearch-mcp.git
cd localsearch-mcp
```

2. Install dependencies:
```bash
uv sync
```

3. Build the Wikipedia index (first run only):
```bash
# Set smaller subset for testing (optional)
export WIKI_SUBSET_SIZE=10000  # Default: 1,000,000

uv run python -m src
# Press Ctrl+C after index is built
```

This will download English Wikipedia and create:
- **BM25 index** (keyword search) in `data/wiki_index.pkl`
- **Vector index** (semantic search) in `data/chroma_db/`

The initial build downloads documents and generates embeddings, which takes time. Default: 1M articles (~5GB). Full dataset: 6.8M articles (~20GB).

4. (Optional) Enable local file search:
```bash
# Set the path to your local documents
export LOCAL_DOCS_PATH="/path/to/your/notes"  # e.g., ~/ObsidianVault/Research
```

This enables searching through your:
- Markdown files (`.md`)
- Text files (`.txt`)
- Any personal notes or documentation

The server will scan this directory on each startup to index the latest content.

## Usage

### Running the MCP Server

```bash
# Without local files
uv run python -m src

# With local files
LOCAL_DOCS_PATH="/path/to/your/notes" uv run python -m src
```

The server will:
1. Load the pre-built Wikipedia index (cached, fast)
2. Scan and index local files if `LOCAL_DOCS_PATH` is set (quick for typical document collections)
3. Start listening for MCP requests on stdio
4. Provide search tools: `search`, `search_wikipedia`, and `search_local`

### Testing with Ollama

#### Simple Test (Wikipedia Search, No LLM)
```bash
uv run tests/verify_with_ollama.py --simple
```

This tests the MCP connection and performs a direct Wikipedia search.

#### Local Document Search Test (No LLM)
```bash
uv run tests/verify_with_ollama.py --local
```

This tests the local file search capability with domain-specific queries. By default, it uses VisionSort/Casper KB documents as the test dataset.

Example output:
```
ğŸ§ª Running Local Document Search Test (VisionSort/Casper KB)...
ğŸ“ Local docs path: /Users/ikmx/source/tc/Casper_KB-main

âœ… Available tools: ['search', 'search_wikipedia', 'search_local']

--- Test 1: VisionSort 405nmãƒ¬ãƒ¼ã‚¶ãƒ¼ã®å‡ºåŠ› ---
ğŸ” Query: VisionSort 405nm laser output power mW
ğŸ“‹ Expected: 365 mW
âœ… PASS: Expected answer found in results!

--- Test 2: ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰4015ã®æ„å‘³ã¨å¯¾å‡¦æ³• ---
ğŸ” Query: FluidicSystem error code 4015 CL Leak
ğŸ“‹ Expected: Emergency level, chip holder leak
âœ… PASS: Related document found!
```

#### Full Agent Test (Requires Ollama)
```bash
# Make sure Ollama is running with a tool-compatible model
ollama pull llama3.2
ollama serve

# In another terminal:
uv run tests/verify_with_ollama.py
```

Expected output:
```
ğŸ¤– Starting MCP Client and connecting to Local Search Server...
âœ… Connected. Available tools: ['search', 'search_wikipedia', 'search_local']

ğŸ‘¤ User Query: Pythonã¨ã„ã†ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã®æ­´å²ã«ã¤ã„ã¦ã€ç°¡æ½”ã«æ•™ãˆã¦
ğŸ› ï¸  Agent requested 1 tool call(s)
   â†’ Tool: search_wikipedia
   â†’ Args: {'query': 'history of python programming language'}
   â†’ Output length: 1523 chars

ğŸ¤– Agent Answer:
Python was created by Guido van Rossum in the late 1980s...
```

### Integration with Claude Desktop

Add this to your Claude Desktop MCP configuration:

**Wikipedia only:**
```json
{
  "mcpServers": {
    "local-search": {
      "command": "uv",
      "args": ["run", "python", "-m", "src"],
      "cwd": "/path/to/localsearch-mcp"
    }
  }
}
```

**Wikipedia + Local Files:**
```json
{
  "mcpServers": {
    "local-search": {
      "command": "uv",
      "args": ["run", "python", "-m", "src"],
      "cwd": "/path/to/localsearch-mcp",
      "env": {
        "LOCAL_DOCS_PATH": "/Users/yourname/Documents/Notes"
      }
    }
  }
}
```

Then restart Claude Desktop and you can search both Wikipedia and your personal files in conversations!

## Project Structure

```
localsearch-mcp/
â”œâ”€â”€ pyproject.toml          # Dependencies and project metadata
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ .env.example            # Environment variable configuration example
â”œâ”€â”€ data/                   # Index storage (created on first run)
â”‚   â”œâ”€â”€ .gitkeep
â”‚   â”œâ”€â”€ wiki_index.pkl      # Wikipedia BM25 index (cached)
â”‚   â”œâ”€â”€ chroma_db/          # Wikipedia vector index
â”‚   â””â”€â”€ local_chroma_db/    # Local files vector index
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __main__.py         # Entry point for `python -m src`
â”‚   â”œâ”€â”€ server.py           # MCP server implementation
â”‚   â”œâ”€â”€ indexer.py          # Multi-source hybrid indexing
â”‚   â””â”€â”€ loaders.py          # Local file loaders
â”œâ”€â”€ test_docs/              # Test documents for CI/CD
â”‚   â”œâ”€â”€ document1.md        # Sample documents
â”‚   â”œâ”€â”€ document2.md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ test_notes/             # Additional sample test files
â”‚   â”œâ”€â”€ secret_project.md
â”‚   â””â”€â”€ meeting_notes.md
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md           # Test documentation
â”‚   â”œâ”€â”€ test_indexing_search.py   # CI/CD test suite (no LLM)
â”‚   â””â”€â”€ verify_with_ollama.py     # LLM integration tests (local only)
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â”œâ”€â”€ test.yml        # CI/CD test workflow
        â””â”€â”€ lint.yml        # Code quality checks
```

## Available Tools

### `search` (Multi-Source)

Search across Wikipedia AND your local files simultaneously using hybrid search.

**Parameters:**
- `query` (string, required): Search keywords or question
- `top_k` (integer, optional): Number of results to return per source (default: 5, max: 20)
- `strategy` (string, optional): Search strategy - `"hybrid"` (default), `"keyword"`, or `"semantic"`
- `source` (string, optional): Data source - `"all"` (default), `"wikipedia"`, or `"local"`

**Source Options:**
- **`"all"`** (default): Search both Wikipedia and local files for comprehensive results
- **`"wikipedia"`**: Search only Wikipedia (general knowledge)
- **`"local"`**: Search only your local files (personal knowledge)

**Search Strategies:**
- **`"hybrid"`** (recommended): Combines keyword matching and semantic similarity for best results
- **`"keyword"`**: Traditional BM25 keyword search (exact word matching, fast)
- **`"semantic"`**: Vector similarity search (finds conceptually similar content, even without exact words)

**Returns:**
Formatted search results with titles, URLs/paths, and content snippets. Results from both sources are merged intelligently using Reciprocal Rank Fusion (RRF).

### `search_wikipedia`

Search English Wikipedia only using hybrid search (BM25 + Vector embeddings). Convenience wrapper for `search` with `source="wikipedia"`.

**Parameters:**
- `query` (string, required): Search keywords or question
- `top_k` (integer, optional): Number of results to return (default: 3, max: 10)
- `strategy` (string, optional): Search strategy - `"hybrid"` (default), `"keyword"`, or `"semantic"`

### `search_local`

Search your local files only using hybrid search. Convenience wrapper for `search` with `source="local"`.

**Parameters:**
- `query` (string, required): Search keywords or question
- `top_k` (integer, optional): Number of results to return (default: 5, max: 20)
- `strategy` (string, optional): Search strategy - `"hybrid"` (default), `"keyword"`, or `"semantic"`

**Examples:**
```python
# Hybrid search (best results, default)
result = await session.call_tool(
    "search_wikipedia",
    arguments={"query": "python programming language", "top_k": 3}
)

# Keyword-only search (fast, exact matches)
result = await session.call_tool(
    "search_wikipedia",
    arguments={"query": "python programming language", "strategy": "keyword"}
)

# Semantic search (finds similar concepts)
result = await session.call_tool(
    "search_wikipedia",
    arguments={"query": "snake that inspired a programming language", "strategy": "semantic"}
)
```

## Customization

### Using Simple English Wikipedia (for development)

For faster development/testing, use the lightweight Simple English Wikipedia:

Edit `src/indexer.py`:
```python
# Change this line:
ds = load_dataset("wikimedia/wikipedia", "20231101.en", split="train")

# To (Simple English, limited to 10k articles):
ds = load_dataset("wikimedia/wikipedia", "20231101.simple", split="train[:10000]")
```

This reduces disk space to ~500MB and builds in a few minutes.

### Adjusting Index Size

You can limit the number of articles for testing:

```python
# Limit to 1000 articles
ds = load_dataset("wikimedia/wikipedia", "20231101.en", split="train[:1000]")
```

## Development

### Running Tests

This project has two types of tests:

#### 1. CI/CD Tests (Automated)

These tests run automatically in GitHub Actions and require no LLM:

```bash
# Run the full CI/CD test suite (with local files only, fast)
SKIP_WIKIPEDIA=true uv run python tests/test_indexing_search.py

# Run with Wikipedia indexing (requires ~500MB disk space and internet)
uv run python tests/test_indexing_search.py
```

**What's tested:**
- MCP server connection
- Local document indexing
- Search results quality
- Incremental indexing (mtime-based change detection)
- Search strategies (keyword vs hybrid)

These tests use the `test_docs/` directory containing sample documents in the repository.

#### 2. LLM Integration Tests (Local Only)

These tests require Ollama and are for local development only:

```bash
# Simple MCP connection test (Wikipedia search, no LLM)
uv run python tests/verify_with_ollama.py --simple

# Local document search test (no LLM)
uv run python tests/verify_with_ollama.py --local

# Q&A test with Ollama (requires llama3.2)
uv run python tests/verify_with_ollama.py --local-qa

# Full agent test with function calling (requires llama3.2 and command-r)
uv run python tests/verify_with_ollama.py
```

**Requirements:**
- Ollama installed and running
- Models: `llama3.2`, `command-r` (install with `ollama pull <model>`)

### Test Options

| Test File | Type | LLM Required | Purpose |
|-----------|------|--------------|---------|
| `test_indexing_search.py` | CI/CD | No | Automated testing of core functionality |
| `verify_with_ollama.py --simple` | Manual | No | Basic connection test |
| `verify_with_ollama.py --local` | Manual | No | Local search validation |
| `verify_with_ollama.py --local-qa` | Manual | Yes | Q&A with local docs |
| `verify_with_ollama.py` | Manual | Yes | Full agent workflow |

See `tests/README.md` for detailed test documentation.

### Customizing Local Document Path

Set the `LOCAL_DOCS_PATH` environment variable to use your own documents:

```bash
export LOCAL_DOCS_PATH="/path/to/your/documents"
uv run python tests/test_indexing_search.py
```

### Rebuilding Index
Delete `data/wiki_index.pkl` and restart the server.

## Troubleshooting

### Index Not Building
- Check disk space (needs ~500MB for Simple Wikipedia, ~20GB for full)
- Ensure stable internet connection for initial download
- Check Python version (3.10+ required)

### Ollama Connection Fails
- Verify Ollama is running: `ollama list`
- Ensure a tool-compatible model is installed: `ollama pull command-r`
- Check Ollama API is accessible: `curl http://localhost:11434`

### MCP Server Not Starting
- Check dependencies: `uv sync`
- Verify Python path in MCP config
- Check for port conflicts

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

MIT License - see LICENSE file for details

---

# æ—¥æœ¬èªç‰ˆ

## æ¦‚è¦

ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å‹•ä½œã™ã‚‹ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹æ¤œç´¢ MCP ã‚µãƒ¼ãƒãƒ¼ã§ã™ã€‚**Wikipediaï¼ˆé™çš„ã§å¤§è¦æ¨¡ãªçŸ¥è­˜ï¼‰**ã¨**ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå‹•çš„ã§å€‹äººçš„ãªçŸ¥è­˜ï¼‰**ã®ä¸¡æ–¹ã‚’æ¤œç´¢ã§ãã€å¤–éƒ¨ API ã«ä¾å­˜ã›ãšå®Œå…¨ã«ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§å‹•ä½œã—ã¾ã™ã€‚

## ç‰¹å¾´

- **ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹æ¤œç´¢**: Wikipedia ã¨ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆMarkdownã€ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‚’åŒæ™‚ã«æ¤œç´¢å¯èƒ½
- **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢**: BM25ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼‰+ ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ï¼ˆæ„å‘³æ¤œç´¢ï¼‰ã®çµ„ã¿åˆã‚ã›ã§æœ€é«˜ã®çµæœã‚’æä¾›
- **ã‚¹ãƒãƒ¼ãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹**: Wikipedia ã¯æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¯èµ·å‹•æ™‚ã«ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦æœ€æ–°çŠ¶æ…‹ã‚’åæ˜ 
- **å®Œå…¨ã‚ªãƒ•ãƒ©ã‚¤ãƒ³**: ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šä¸è¦
- **ç„¡æ–™ãƒ»é«˜é€Ÿ**: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨æ„å‘³ã®ä¸¡æ–¹ã«å¯¾å¿œã—ãŸåŠ¹ç‡çš„ãªæ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- **MCP äº’æ›**: Claude Desktop ãªã©ã® MCP å¯¾å¿œã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ä½¿ç”¨å¯èƒ½
- **Ollama çµ±åˆ**: Ollama ã‚’ä½¿ã£ãŸãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä»˜å±
- **ç°¡å˜ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**: `uv` ã«ã‚ˆã‚‹ç°¡å˜ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama    â”‚ â—„â”€â”€â”€â”€â”€â”€ â”‚  MCP Client      â”‚ â—„â”€â”€â”€â”€â”€â”€ â”‚   Human     â”‚
â”‚   (LLM)     â”‚         â”‚  (test script)   â”‚         â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â”‚ MCP Protocol
                                â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  MCP Server      â”‚
                        â”‚  (src/server.py) â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â–¼                         â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Wikipedia Indexerâ”‚      â”‚ Local File       â”‚
         â”‚ (é™çš„/ã‚­ãƒ£ãƒƒã‚·ãƒ¥)â”‚      â”‚ Indexer (å‹•çš„)   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚                         â”‚
                   â–¼                         â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ BM25 + Vector DB â”‚      â”‚ BM25 + Vector DB â”‚
         â”‚ (100ä¸‡ä»¶ä»¥ä¸Š)    â”‚      â”‚ (ã‚ãªãŸã®ãƒ•ã‚¡ã‚¤ãƒ«)â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Composite Pattern**: ä¸¡æ–¹ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®çµæœã‚’ Reciprocal Rank Fusion (RRF) ã§ãƒãƒ¼ã‚¸ã—ã¦æœ€é©ãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å®Ÿç¾

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### å¿…è¦è¦ä»¶

- Python 3.10 ä»¥ä¸Š
- [uv](https://github.com/astral-sh/uv) ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
- (ã‚ªãƒ—ã‚·ãƒ§ãƒ³) ãƒ†ã‚¹ãƒˆç”¨ã® Ollama ã¨ãƒ„ãƒ¼ãƒ«å¯¾å¿œãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹: command-rï¼‰

### ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †

1. ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³:
```bash
git clone https://github.com/yourusername/localsearch-mcp.git
cd localsearch-mcp
```

2. ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:
```bash
uv sync
```

3. Wikipedia ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ï¼ˆåˆå›ã®ã¿ï¼‰:
```bash
# ãƒ†ã‚¹ãƒˆç”¨ã«å°ã•ã„ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
export WIKI_SUBSET_SIZE=10000  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1,000,000

uv run python -m src
# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å¾Œ Ctrl+C ã§çµ‚äº†
```

ã“ã‚Œã«ã‚ˆã‚Šä»¥ä¸‹ãŒä½œæˆã•ã‚Œã¾ã™ï¼š
- **BM25 ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹**ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼‰: `data/wiki_index.pkl`
- **ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹**ï¼ˆæ„å‘³æ¤œç´¢ï¼‰: `data/chroma_db/`

åˆå›æ§‹ç¯‰ã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚’è¡Œã†ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ä¸‡è¨˜äº‹ï¼ˆç´„5GBï¼‰ã€‚å®Œå…¨ç‰ˆ: 680ä¸‡è¨˜äº‹ï¼ˆç´„20GBï¼‰ã€‚

4. (ã‚ªãƒ—ã‚·ãƒ§ãƒ³) ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã‚’æœ‰åŠ¹åŒ–:
```bash
# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ‘ã‚¹ã‚’è¨­å®š
export LOCAL_DOCS_PATH="/path/to/your/notes"  # ä¾‹: ~/ObsidianVault/Research
```

ã“ã‚Œã«ã‚ˆã‚Šä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š
- Markdown ãƒ•ã‚¡ã‚¤ãƒ« (`.md`)
- ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« (`.txt`)
- å€‹äººçš„ãªãƒãƒ¼ãƒˆã‚„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

ã‚µãƒ¼ãƒãƒ¼ã¯èµ·å‹•æ™‚ã«ã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦æœ€æ–°ã®å†…å®¹ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã—ã¾ã™ã€‚

## ä½¿ã„æ–¹

### MCP ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•

```bash
# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãªã—ã§èµ·å‹•
uv run python -m src

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚ã‚Šã§èµ·å‹•
LOCAL_DOCS_PATH="/path/to/your/notes" uv run python -m src
```

ã‚µãƒ¼ãƒãƒ¼ã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
1. æ§‹ç¯‰æ¸ˆã¿ Wikipedia ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰é«˜é€Ÿèª­ã¿è¾¼ã¿ï¼‰
2. `LOCAL_DOCS_PATH` ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ï¼ˆé€šå¸¸ã¯æ•°ç§’ï¼‰
3. æ¨™æº–å…¥å‡ºåŠ›ã§ MCP ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å¾…æ©Ÿ
4. æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’æä¾›: `search`ã€`search_wikipedia`ã€`search_local`

### Ollama ã‚’ä½¿ã£ãŸãƒ†ã‚¹ãƒˆ

#### ã‚·ãƒ³ãƒ—ãƒ«ãƒ†ã‚¹ãƒˆï¼ˆWikipedia æ¤œç´¢ã€LLM ãªã—ï¼‰
```bash
uv run tests/verify_with_ollama.py --simple
```

MCP æ¥ç¶šã¨ Wikipedia æ¤œç´¢æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚

#### ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ãƒ†ã‚¹ãƒˆï¼ˆLLM ãªã—ï¼‰
```bash
uv run tests/verify_with_ollama.py --local
```

ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢æ©Ÿèƒ½ã‚’ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®ã‚¯ã‚¨ãƒªã§ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ VisionSort/Casper KB ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚

å‡ºåŠ›ä¾‹:
```
ğŸ§ª Running Local Document Search Test (VisionSort/Casper KB)...
ğŸ“ Local docs path: /Users/ikmx/source/tc/Casper_KB-main

âœ… Available tools: ['search', 'search_wikipedia', 'search_local']

--- Test 1: VisionSort 405nmãƒ¬ãƒ¼ã‚¶ãƒ¼ã®å‡ºåŠ› ---
ğŸ” Query: VisionSort 405nm laser output power mW
ğŸ“‹ Expected: 365 mW
âœ… PASS: Expected answer found in results!
```

#### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ†ã‚¹ãƒˆï¼ˆOllama å¿…è¦ï¼‰
```bash
# Ollama ã¨ llama3.2 ãƒ¢ãƒ‡ãƒ«ã‚’èµ·å‹•
ollama pull llama3.2
ollama serve

# åˆ¥ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§å®Ÿè¡Œ:
uv run tests/verify_with_ollama.py
```

### Claude Desktop ã¨ã®çµ±åˆ

Claude Desktop ã® MCP è¨­å®šã«ä»¥ä¸‹ã‚’è¿½åŠ :

**Wikipedia ã®ã¿:**
```json
{
  "mcpServers": {
    "local-search": {
      "command": "uv",
      "args": ["run", "python", "-m", "src"],
      "cwd": "/path/to/localsearch-mcp"
    }
  }
}
```

**Wikipedia + ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«:**
```json
{
  "mcpServers": {
    "local-search": {
      "command": "uv",
      "args": ["run", "python", "-m", "src"],
      "cwd": "/path/to/localsearch-mcp",
      "env": {
        "LOCAL_DOCS_PATH": "/Users/yourname/Documents/Notes"
      }
    }
  }
}
```

Claude Desktop ã‚’å†èµ·å‹•ã™ã‚‹ã¨ã€ä¼šè©±å†…ã§ Wikipedia ã¨å€‹äººãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸¡æ–¹ã‚’æ¤œç´¢ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼

## åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«

### `search` (ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹)

Wikipedia ã¨ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸¡æ–¹ã‚’ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã§åŒæ™‚ã«æ¤œç´¢ã—ã¾ã™ã€‚

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**
- `query` (æ–‡å­—åˆ—, å¿…é ˆ): æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¾ãŸã¯è³ªå•
- `top_k` (æ•´æ•°, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): ã‚½ãƒ¼ã‚¹ã”ã¨ã«è¿”ã™çµæœã®æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ã€æœ€å¤§: 20ï¼‰
- `strategy` (æ–‡å­—åˆ—, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): æ¤œç´¢æˆ¦ç•¥ - `"hybrid"` (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)ã€`"keyword"`ã€ã¾ãŸã¯ `"semantic"`
- `source` (æ–‡å­—åˆ—, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ - `"all"` (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)ã€`"wikipedia"`ã€ã¾ãŸã¯ `"local"`

**ã‚½ãƒ¼ã‚¹ã‚ªãƒ—ã‚·ãƒ§ãƒ³:**
- **`"all"`** (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ): Wikipedia ã¨ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸¡æ–¹ã‚’æ¤œç´¢ã—ã¦åŒ…æ‹¬çš„ãªçµæœã‚’å–å¾—
- **`"wikipedia"`**: Wikipedia ã®ã¿æ¤œç´¢ï¼ˆä¸€èˆ¬çŸ¥è­˜ï¼‰
- **`"local"`**: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿æ¤œç´¢ï¼ˆå€‹äººçŸ¥è­˜ï¼‰

**æ¤œç´¢æˆ¦ç•¥:**
- **`"hybrid"`** (æ¨å¥¨): ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã¨æ„å‘³æ¤œç´¢ã‚’çµ„ã¿åˆã‚ã›ã¦æœ€è‰¯ã®çµæœã‚’æä¾›
- **`"keyword"`**: å¾“æ¥ã® BM25 ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆå®Œå…¨ä¸€è‡´ã€é«˜é€Ÿï¼‰
- **`"semantic"`**: ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦æ¤œç´¢ï¼ˆå˜èªãŒä¸€è‡´ã—ãªãã¦ã‚‚æ¦‚å¿µçš„ã«é¡ä¼¼ã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¤œç´¢ï¼‰

**æˆ»ã‚Šå€¤:**
ã‚¿ã‚¤ãƒˆãƒ«ã€URL/ãƒ‘ã‚¹ã€æœ¬æ–‡ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’å«ã‚€æ¤œç´¢çµæœã€‚ä¸¡æ–¹ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®çµæœã¯ Reciprocal Rank Fusion (RRF) ã§ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã«ãƒãƒ¼ã‚¸ã•ã‚Œã¾ã™ã€‚

### `search_wikipedia`

Wikipedia ã®ã¿ã‚’ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆBM25 + ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ï¼‰ã§æ¤œç´¢ã—ã¾ã™ã€‚`search` ã® `source="wikipedia"` ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã§ã™ã€‚

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**
- `query` (æ–‡å­—åˆ—, å¿…é ˆ): æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¾ãŸã¯è³ªå•
- `top_k` (æ•´æ•°, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): è¿”ã™çµæœã®æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ã€æœ€å¤§: 10ï¼‰
- `strategy` (æ–‡å­—åˆ—, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): æ¤œç´¢æˆ¦ç•¥ - `"hybrid"` (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)ã€`"keyword"`ã€ã¾ãŸã¯ `"semantic"`

### `search_local`

ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã§æ¤œç´¢ã—ã¾ã™ã€‚`search` ã® `source="local"` ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã§ã™ã€‚

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**
- `query` (æ–‡å­—åˆ—, å¿…é ˆ): æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¾ãŸã¯è³ªå•
- `top_k` (æ•´æ•°, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): è¿”ã™çµæœã®æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ã€æœ€å¤§: 20ï¼‰
- `strategy` (æ–‡å­—åˆ—, ã‚ªãƒ—ã‚·ãƒ§ãƒ³): æ¤œç´¢æˆ¦ç•¥ - `"hybrid"` (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)ã€`"keyword"`ã€ã¾ãŸã¯ `"semantic"`

## ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### å®Œå…¨ç‰ˆ Wikipedia ã‚’ä½¿ç”¨

`src/indexer.py` ã‚’ç·¨é›†:
```python
# ã“ã®è¡Œã‚’å¤‰æ›´:
ds = load_dataset("wikipedia", "20220301.simple", split="train[:10000]")

# ä»¥ä¸‹ã«å¤‰æ›´:
ds = load_dataset("wikipedia", "20231101.en", split="train")
```

æ³¨: ç´„20GB ã®ãƒ‡ã‚£ã‚¹ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã¨é•·ã„æ§‹ç¯‰æ™‚é–“ãŒå¿…è¦ã§ã™ã€‚

## é–‹ç™º

### ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã¯2ç¨®é¡ã®ãƒ†ã‚¹ãƒˆãŒã‚ã‚Šã¾ã™ï¼š

#### 1. CI/CD ãƒ†ã‚¹ãƒˆï¼ˆè‡ªå‹•åŒ–ï¼‰

GitHub Actions ã§è‡ªå‹•çš„ã«å®Ÿè¡Œã•ã‚Œã€LLM ã¯ä¸è¦ã§ã™ï¼š

```bash
# CI/CD ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®å®Ÿè¡Œï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã€é«˜é€Ÿï¼‰
SKIP_WIKIPEDIA=true uv run python tests/test_indexing_search.py

# Wikipedia ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚ã‚Šã§å®Ÿè¡Œï¼ˆç´„500MBã®ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šãŒå¿…è¦ï¼‰
uv run python tests/test_indexing_search.py
```

**ãƒ†ã‚¹ãƒˆå†…å®¹:**
- MCP ã‚µãƒ¼ãƒãƒ¼æ¥ç¶š
- ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
- æ¤œç´¢çµæœã®å“è³ª
- å¢—åˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆmtime ãƒ™ãƒ¼ã‚¹ã®å¤‰æ›´æ¤œå‡ºï¼‰
- æ¤œç´¢æˆ¦ç•¥ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ vs ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼‰

ã“ã‚Œã‚‰ã®ãƒ†ã‚¹ãƒˆã¯ãƒªãƒã‚¸ãƒˆãƒªå†…ã® `test_docs/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

#### 2. LLM çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã®ã¿ï¼‰

Ollama ãŒå¿…è¦ã§ã€ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºå°‚ç”¨ã§ã™ï¼š

```bash
# ã‚·ãƒ³ãƒ—ãƒ«ãª MCP æ¥ç¶šãƒ†ã‚¹ãƒˆï¼ˆWikipedia æ¤œç´¢ã€LLM ãªã—ï¼‰
uv run python tests/verify_with_ollama.py --simple

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ãƒ†ã‚¹ãƒˆï¼ˆLLM ãªã—ï¼‰
uv run python tests/verify_with_ollama.py --local

# Ollama ã‚’ä½¿ã£ãŸ Q&A ãƒ†ã‚¹ãƒˆï¼ˆllama3.2 ãŒå¿…è¦ï¼‰
uv run python tests/verify_with_ollama.py --local-qa

# é–¢æ•°å‘¼ã³å‡ºã—ã‚’ä½¿ã£ãŸå®Œå…¨ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ†ã‚¹ãƒˆï¼ˆllama3.2 ã¨ command-r ãŒå¿…è¦ï¼‰
uv run python tests/verify_with_ollama.py
```

**å¿…è¦æ¡ä»¶:**
- Ollama ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨èµ·å‹•
- ãƒ¢ãƒ‡ãƒ«: `llama3.2`ã€`command-r` (`ollama pull <model>` ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)

### ãƒ†ã‚¹ãƒˆã‚ªãƒ—ã‚·ãƒ§ãƒ³

| ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« | ã‚¿ã‚¤ãƒ— | LLM å¿…è¦ | ç›®çš„ |
|--------------|------|----------|------|
| `test_indexing_search.py` | CI/CD | ä¸è¦ | ã‚³ã‚¢æ©Ÿèƒ½ã®è‡ªå‹•ãƒ†ã‚¹ãƒˆ |
| `verify_with_ollama.py --simple` | æ‰‹å‹• | ä¸è¦ | åŸºæœ¬æ¥ç¶šãƒ†ã‚¹ãƒˆ |
| `verify_with_ollama.py --local` | æ‰‹å‹• | ä¸è¦ | ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã®æ¤œè¨¼ |
| `verify_with_ollama.py --local-qa` | æ‰‹å‹• | å¿…è¦ | ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã® Q&A |
| `verify_with_ollama.py` | æ‰‹å‹• | å¿…è¦ | å®Œå…¨ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ |

è©³ç´°ãªãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ `tests/README.md` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

### ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‘ã‚¹ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

`LOCAL_DOCS_PATH` ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ã€ç‹¬è‡ªã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½¿ç”¨ã§ãã¾ã™ï¼š

```bash
export LOCAL_DOCS_PATH="/path/to/your/documents"
uv run python tests/test_indexing_search.py
```

### ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å†æ§‹ç¯‰
`data/wiki_index.pkl` ã‚’å‰Šé™¤ã—ã¦ã‚µãƒ¼ãƒãƒ¼ã‚’å†èµ·å‹•ã—ã¾ã™ã€‚

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ§‹ç¯‰ã•ã‚Œãªã„
- ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã‚’ç¢ºèªï¼ˆSimple ç‰ˆã§ç´„500MBã€å®Œå…¨ç‰ˆã§ç´„20GBå¿…è¦ï¼‰
- åˆå›ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèª
- Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªï¼ˆ3.10ä»¥ä¸Šå¿…è¦ï¼‰

### Ollama æ¥ç¶šã‚¨ãƒ©ãƒ¼
- Ollama ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèª: `ollama list`
- llama3.2 ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª: `ollama pull llama3.2`
- Ollama API ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹ç¢ºèª: `curl http://localhost:11434`

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT License
